{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c05628-c946-421d-8dff-95c5410d5e14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample ARC Submission\n",
    "\n",
    "This is a sample notebook that can help get you started with creating an ARC Prize submission. It covers the basics of loading libraries, loading data, implementing an approach, and submitting.\n",
    "\n",
    "You should be able to submit this notebook to the evaluation portal and have it run successfully (although you'll get a score of 0, so you'll need to do some work if you want to do better!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717270ed-182b-4598-8f12-612cd62de60e",
   "metadata": {},
   "source": [
    "# Load needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828dd6cc-7da7-4bec-a211-ff9afb2e688a",
   "metadata": {},
   "source": [
    "Basic libraries like numpy, torch, matplotlib, and tqdm are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3d361d-d3e0-45d2-b416-803c0ad1822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd78b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae22af4-5a95-40d1-ac1f-f611b77578c7",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4bf5a-3237-46f6-ab05-2319a0497a8b",
   "metadata": {},
   "source": [
    "Here we are loading the training challenges and solutions (this is the public training set), the evaluation challenges and solutions (this is the public evaluation set), and the test challenges (currently a placeholder file that is a copy of the public evaluation challanges).\n",
    "\n",
    "For your initial testing and exploration, I'd recommend not using the public evaluation set, just work off the public training set and then test against the test challenges (which is actually the public evaluation set). However, when competing in the competition, then you can should probably use the evaluation tasks for training too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207605ea-58a7-46e0-9fd2-3ee2000eece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public training set\n",
    "train_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(train_challenges_path) as fp:\n",
    "    train_challenges = json.load(fp)\n",
    "with open(train_solutions_path) as fp:\n",
    "    train_solutions = json.load(fp)\n",
    "\n",
    "# Public evaluation set\n",
    "evaluation_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "evaluation_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(evaluation_challenges_path) as fp:\n",
    "    evaluation_challenges = json.load(fp)\n",
    "with open(evaluation_solutions_path) as fp:\n",
    "    evaluation_solutions = json.load(fp)\n",
    "\n",
    "# This will be the hidden test challenges (currently has a placeholder to the evaluation set)\n",
    "test_challenges_path = '../input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "\n",
    "with open(test_challenges_path) as fp:\n",
    "    test_challenges = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292806c-8c0c-4efd-9c30-b3660254e4c2",
   "metadata": {},
   "source": [
    "Here is an example of what a test task looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be570a8b-7105-4935-b538-5e58794406f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [{'input': [[3, 2], [7, 8]]}],\n",
       " 'train': [{'input': [[8, 6], [6, 4]],\n",
       "   'output': [[8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4],\n",
       "    [6, 8, 6, 8, 6, 8],\n",
       "    [4, 6, 4, 6, 4, 6],\n",
       "    [8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4]]},\n",
       "  {'input': [[7, 9], [4, 3]],\n",
       "   'output': [[7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3],\n",
       "    [9, 7, 9, 7, 9, 7],\n",
       "    [3, 4, 3, 4, 3, 4],\n",
       "    [7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3]]}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_task = list(test_challenges.keys())[0]\n",
    "challenge_ex = test_challenges[sample_task]\n",
    "challenge_ex\n",
    "# sample_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f34083-882b-4fb9-b5fd-92dd75772dc3",
   "metadata": {},
   "source": [
    "# Generating a submission\n",
    "\n",
    "To generate a submission you need to output a file called `submission.json` that has the following format:\n",
    "\n",
    "```\n",
    "{\"00576224\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"009d5c81\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"12997ef3\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]},\n",
    "              {\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this case, the task ids come from `test_challenges`. There may be multiple (i.e., >1) test items per task. Therefore, the dictionary has a list of dicts for each task. These submission dictionaries should appear in the same order as the test items from `test_challenges`. Additionally, you can provide two attempts for each test item. In fact, you **MUST** provide two attempts. If you only want to generate a single attempt, then just submit the same answer for both attempts (or submit an empty submission like the ones shown in the example snippit just above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd1f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_bfs import arc_search, execute_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e25a0fb-e206-4505-b597-8863c41c7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:36<00:00, 10.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty submission dict for output\n",
    "submission = {}\n",
    "\n",
    "# iterate over the test items and build up submission answers\n",
    "count = 0\n",
    "for key, task in tqdm(test_challenges.items()):\n",
    "\n",
    "    # Here are the task's training inputs and outputs\n",
    "    train_inputs = [item['input'] for item in task['train']]\n",
    "    train_outputs = [item['output'] for item in task['train']]\n",
    "    \n",
    "    test_inputs = [item['input'] for item in task['test']]\n",
    "\n",
    "    # Here we generate outputs for each test item.\n",
    "    submission[key] = []\n",
    "\n",
    "    transformations = set()\n",
    "    for input_grid, output_grid in zip(train_inputs, train_outputs):\n",
    "        transformations.add(tuple(arc_search(input_grid, output_grid, max_depth=2)))\n",
    "    \n",
    "    transformations = sorted(list(transformations), key=len, reverse=True) # longest solutions come first to avoid empty lists\n",
    "\n",
    "    for input_grid in test_inputs:\n",
    "        attempt_dict = dict()\n",
    "        for i in range(2):\n",
    "            trans = transformations[i % len(transformations)]\n",
    "            soln = execute_transformation(input_grid, trans)\n",
    "            attempt_dict[f'attempt_{i+1}'] = soln\n",
    "            # if soln != input_grid:\n",
    "            #     print(\"CHANGED SOMETHING!!\")\n",
    "        submission[key].append(attempt_dict)\n",
    "            \n",
    "# Here we write the submissions to file, so that they will get evaluated\n",
    "with open('submission.json', 'w') as fp:\n",
    "    json.dump(submission, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3d827-8f78-47d4-8bc7-3c07cfc996c2",
   "metadata": {},
   "source": [
    "Here is what our submission for the test task above looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d83daa-496f-4cad-8550-876d115c4a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'attempt_1': [[3, 2], [7, 8]], 'attempt_2': [[3, 2], [7, 8]]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[sample_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7599",
   "metadata": {},
   "source": [
    "# Scoring Your Submission\n",
    "\n",
    "If you do not want to wait for gradescope to score your solution, we have provided the following code to score your submission. Note that the maximum possibe score is 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39df0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_submission():\n",
    "    with open('../input/arc-prize-2024/arc-agi_evaluation_solutions.json', 'r') as sol_file:\n",
    "        solutions = json.load(sol_file)\n",
    "    \n",
    "    with open('submission.json', 'r') as sub_file:\n",
    "        submission = json.load(sub_file)\n",
    "    \n",
    "    overall_score = 0\n",
    "\n",
    "    for task in solutions:\n",
    "        score = 0\n",
    "        for i, answer in enumerate(solutions[task]):\n",
    "            attempt1_correct = submission[task][i]['attempt_1'] == answer\n",
    "            attempt2_correct = submission[task][i]['attempt_2'] == answer\n",
    "            score += int(attempt1_correct or attempt2_correct)\n",
    "\n",
    "        score /= len(solutions[task])\n",
    "\n",
    "        overall_score += score\n",
    "    \n",
    "\n",
    "    print(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6818bb2",
   "metadata": {},
   "source": [
    "You can run the above code by uncommenting the following code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b131fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "score_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6466c66-c324-44c2-8e05-3860e0fe1fdd",
   "metadata": {},
   "source": [
    "# Confused about where to get started? \n",
    "\n",
    "If you're not sure what an initial solution might look like, then consider looking at public notebooks here: https://www.kaggle.com/competitions/arc-prize-2024/code or joining the public discussion here: https://www.kaggle.com/competitions/arc-prize-2024/discussion.\n",
    "\n",
    "One example notebook that uses a very simple knowledge-based approach is this one: https://www.kaggle.com/code/michaelhodel/program-synthesis-starter-notebook/notebook, which conducts search over a space of domain specific block languages to form hypotheses and then applies these to test items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
